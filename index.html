<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0030)https://pengsongyou.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weifeng Lin</title>
  
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">	
  <meta name="viewport" content="‚Äúwidth=800‚Äù">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">

  <style type="text/css">
    @import url(http://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300,100italic,100);
      /* Color scheme stolen from Sergey Karayev */
      a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th,tr,p,a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
      }
      strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
      }
      heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
      }
      papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight:500;
      }
      name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
      }
      .one
      {
      width: 160px;
      height: 140px;
      position: relative;
      }
      .two
      {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
      }
      .fade {
       transition: opacity .2s ease-in-out;
       -moz-transition: opacity .2s ease-in-out;
       -webkit-transition: opacity .2s ease-in-out;
      }
      span.highlight {
          background-color: #ffffd0;
      }
    </style>
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tbody><tr>
        <td width="70%" valign="middle">
          <p align="center">
            <name>Weifeng Lin (ÊûóÁÇú‰∏∞)</name>
          </p>
          <p>
            I am currently a PhD candidate at the <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory (MMLab)</a>, The Chinese University of Hong Kong, supervised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a>. 
            I earned both my Bachelor's and Master's degrees from the <a href="https://www.scut.edu.cn/en//">South China University of Technology</a> in 2021 and 2024, respectively, where I had the privilege of being advised by <a href="http://www.dlvc-lab.net/lianwen/">Prof. Lianwen Jin</a> and enjoyed a memorable seven years.
          </p>
          <p>
            My research interests include Computer Vision (CV), Multimodal Large Language Models, and Generative Models. 
            I am deeply committed to contributing to open-source projects, as I firmly believe they are a cornerstone for the sustainable growth of the AI community.
          </p>
              
          <p align="center">
            <a href="wflin37@gmail.com" style="font-size: 16px;">Email: wflin37@gmail.com</a> &nbsp/&nbsp
            <a href="https://scholar.google.com/citations?user=8iCZxIAAAAAJ&hl=zh-CN" style="font-size: 16px;">Google Scholar</a> &nbsp/&nbsp
            <a href="https://github.com/Afeng-x" style="font-size: 16px;">Github</a>
          </p>
          </td>

          <td width="33%">
            <a href="images/avater.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avater.jpg" class="hoverZoomLink"></a>
          </td>
        </tr>
        </tbody></table>
 
  <!-- Preprint -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Preprint</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/pixwizard.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a
            href="https://arxiv.org/abs/2409.15278">
            <papertitle style="font-size: 16px;">PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions</papertitle>
          </a>
          <br>
          <strong>Weifeng Lin</strong>,
          Xinyu Wei, 
          Renrui Zhang, 
          Le Zhuo, 
          Shitian Zhao, 
          Siyuan Huang, 
          Junlin Xie, 
          Yu Qiao, 
          Peng Gao, 
          Hongsheng Li
          <p>
            <em>arxiv Preprint</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2409.15278" style="font-size: 15px;">[Paper] </a> |
            <a href="https://github.com/AFeng-x/PixWizard" style="font-size: 15px;">[Code]</a>
            <br>
            <strong>Keywords:</strong> Image Generation, Image-to-Image, Instruction-based Visual Assistant
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/DnU.png' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a
            href="https://arxiv.org/abs/2403.20271">
            <papertitle style="font-size: 16px;">Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</papertitle>
          </a>
          <br>
          <strong>Weifeng Lin</strong>,
          Xinyu Wei, 
          Renrui Zhang, 
          Ruichuan An, 
          Peng Gao, 
          Bocheng Zou, 
          Yulin Luo, 
          Siyuan Huang, 
          Shanghang Zhang, 
          Hongsheng Li
          <p>
            <em>arxiv Preprint</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2403.20271" style="font-size: 15px;">[Paper] </a> |
            <a href="https://github.com/AFeng-x/Draw-and-Understand" style="font-size: 15px;">[Code]</a>
            <br>
            <strong>Keywords:</strong> MLLM, Visual prompts
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/mGPT.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a
            href="https://arxiv.org/abs/2408.02657">
            <papertitle style="font-size: 16px;">Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining</papertitle>
          </a>
          <br>
          Dongyang Liu*,
          Shitian Zhao*, 
          Le Zhuo*, 
          <strong>Weifeng Lin*</strong>, 
          Yu Qiao, 
          Hongsheng Li, 
          Peng Gao
          <p>
            <em>arxiv Preprint</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2408.02657" style="font-size: 15px;">[Paper] </a> |
            <a href="https://github.com/Alpha-VLLM/Lumina-mGPT" style="font-size: 15px;">[Code]</a>
            <br>
            <strong>Keywords:</strong> Multimodal Autoregressive Generation models
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/hst.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a
            href="https://arxiv.org/abs/2310.05393">
            <papertitle style="font-size: 16px;">Hierarchical Side-Tuning for Vision Transformers</papertitle>
          </a>
          <br>
          <strong>Weifeng Lin</strong>,
          Ziheng Wu,
          Jiayu Chen,
          Wentao Yang,
          Mingxin Huang,
          Jun Huang,
          Lianwen Jin
          <p>
            <em>arxiv Preprint</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2310.05393" style="font-size: 15px;">[paper] </a> |
            <a href="https://github.com/AFeng-x/HST" style="font-size: 15px;">[Code] </a>
            <br>
            <strong>Keywords:</strong> Parameter-efficient transfer learning for vision Transformers in various vision tasks (classification, detection, segmentation)
          </p>
        </td>
      </tr>
    </tbody>
  </table>


  <!-- Publications -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/smt.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a
            href="https://arxiv.org/abs/2307.08579">
            <papertitle style="font-size: 16px;">Scale-Aware Modulation Meet Transformer</papertitle>
          </a>
          <br>
          <strong>Weifeng Lin</strong>,
          Ziheng Wu,
          Jiayu Chen,
          Jun Huang,
          Lianwen Jin
          <p>
            <em>International Conference on Computer Vision <strong>(ICCV)</strong></em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2307.08579" style="font-size: 15px;">[paper] </a> |
            <a href="https://github.com/AFeng-x/SMT" style="font-size: 15px;">[Code] </a>
            <br>
            <strong>Keywords:</strong> Hybrid CNN-Transformer Network (Vision Transformer Backbone)
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/M2SD.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a
            href="https://ojs.aaai.org/index.php/AAAI/article/view/28129">
            <papertitle style="font-size: 16px;">M2SD: Multiple Mixing Self-Distillation for Few-Shot Class-Incremental Learning</papertitle>
          </a>
          <br>
          Jinhao Lin,
          Ziheng Wu,
          <strong>Weifeng Lin</strong>,
          Ziheng Wu,
          Jun Huang,
          RongHua Luo
          <p>
            <em>Association for the Advancement of Artificial Intelligence <strong>(AAAI)</strong></em>, 2024
            <br>
            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28129" style="font-size: 15px;">[paper] </a>
            <br>
            <strong>Keywords:</strong> Few-shot Class-incremental learning
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
	    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/rapid_diff.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
					<a href="https://aclanthology.org/2023.acl-industry.28.pdf">
            <papertitle style="font-size: 16px;">Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed</papertitle>
          </a>
          <br>
            Bingyan Liu*,
            <strong>Weifeng Lin*</strong>,
            Zhongjie Duan,
            Chengyu Wang,
            Ziheng Wu,
            Zipeng Zhang,
            Kui Jia,
            Lianwen Jin,
            Cen Chen,
            Jun Huang
          <p>
            <em>Annual Meeting of the Association for Computational Linguistics <strong>(ACL-Industry)</strong></em>, 2023  
            <br>
            <a href="https://aclanthology.org/2023.acl-industry.28.pdf" style="font-size: 15px;">[paper] </a> |
            <a href="https://github.com/alibaba/EasyNLP" style="font-size: 15px;">[Code] </a>
            <strong>Keywords:</strong> Text-to-image latent diffusion models with rich entity knowledge.
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
	    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/mobile-NAS.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
					<a href="">
            <papertitle style="font-size: 16px;">Building A Mobile Text Recognizer via Truncated SVD-based Knowledge Distillation-Guided NAS</papertitle>
          </a>
          <br>
            <strong>Weifeng Lin</strong>,
            Canyu Xie,
            Dezhi Peng,
            Jiapeng Wang,
            Lianwen Jin,
            Wei Ding,
            Lianwen Jin,
            Cong Yao,
            Mengchao He
          <p>
            <em>British Machine Vision Conference <strong>(BMVC)</strong></em>, 2023  
            <br>
							<a href="https://papers.bmvc2023.org/0375.pdf" style="font-size: 15px;">[Paper]</a> |
              <a href="https://www.modelscope.cn/models/damo/cv_proxylessnas_ocr-detection-db-line-level_damo/summary" style="font-size: 15px;">[Detection] </a> |
              <a href="https://modelscope.cn/models/damo/cv_LightweightEdge_ocr-recognitoin-general_damo/summary" style="font-size: 15px;">[Recognition] </a> |
              <a href="https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/LiteWeightOCR" style="font-size: 15px;">[Demo] </a>
              <strong>Keywords:</strong> Mobile Text Recognizer
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- Experience -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Experience</heading>
          <!-- <p>
            I am devoted to Open Source projects.
          </p> -->
        </td>
      </tr>
    </tbody>
  </table>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/SHAILab.png' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a href="http://opengvlab.shlab.org.cn/">
            <papertitle style="font-size: 16px;">Shanghai AI Lab</papertitle>
          </a>
          <p>
            <strong>Position:</strong> Research Intern, OpenGVLab
            <br>
            <strong>Location:</strong> Shanghai, China
            <br>
            <strong>Time:</strong> 2024.01 - 2024.06
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td width="30%">
          <div class="one">
            <img src='images/ali_cloud.jpg' width="215">
          </div>
        </td>
        <td valign="top" width="70%">
          <a href="https://www.alibabacloud.com/">
            <papertitle style="font-size: 16px;">Alibaba Cloud Intelligent</papertitle>
          </a>
          <p>
            <strong>Position:</strong> Research Intern, Machine Learning - Deep Learning Algorithms
            <br>
            <strong>Location:</strong> Hangzhou, China
            <br>
            <strong>Time:</strong> 2022.11 - 2023.12
          </p>
        </td>
      </tr>
    </tbody>
  </table>


<table width="50%" align="center" border="0" cellpadding="20">
  <tbody>
    <tr>
      <td width="50%" valign="center"></td>
      <script type="text/javascript" id="clustrmaps"
        src="//clustrmaps.com/map_v2.js?d=zEuGs9CDAXseJAka9KDNEQckDn_266DHUDK8xaS2Qu0&cl=ffffff&w=a"></script>
      </td>
    </tr>
  </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td>
    <br>
    <p align="right">
      <font size="2">
      template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
      <!-- <br> -->
      <!-- Last updated: Mar 2023 -->
    </font>
    </p>
    </td>
  </tr>
  </table>



</body>
</html>
